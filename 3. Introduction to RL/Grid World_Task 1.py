
#Grid World â€“ Task 1:
#Small Grid World: 2x2
#- Go from a starting point (S) to a target location (T)
#- Possible actions: up, down, left or right (crashing into the boundary â†’ position remains the same, but
#counts as new step for reward calculation)
#- Rewards:
#ğ’™ğ’•+ğŸ ğ’“ğ’•+ğŸ(ğ’™ğ’•+ğŸ)
#E -1
#T +100
#B -0.5
#S -1

#Task 1:
#â€¢ Perform the first iteration of Q-Learning (Steps 1-5) by hand
#â€¢ Use learning rate ğ›¼ = 0.1 and discount factor ğ›¾ = 1
#â€¢ Neglect epsilon-greedy strategy
# The agent does not explore new actions and always chooses the best-known action
#1. Initialize Q-Table
#2. Choose an action:
#â€¢ Go to current state (S)
#â€¢ All actions have the same Q-value â†’ choose randomly: â€œrightâ€œ
#3. Perform an action: ğ‘¥ğ‘¡+1 = (B)
#4. Measure reward: r = -0.5
#5. Update Q-Table:
#â€¢ Go to new state: (B) â†’ highest Q-value: 0
#â€¢ Compute updated Q-Value for (S | right)

import numpy as np

# Define the grid world 
# Dictionury, Possible actions and the resulting next state.
#Since all Q-values are initially 0, the action is chosen randomly.
grid = {
    'S': {'reward': -1, 'actions': {'up': 'E', 'down': 'S', 'left': 'S', 'right': 'B'}},
    'E': {'reward': -1, 'actions': {'up': 'E', 'down': 'S', 'left': 'E', 'right': 'E'}},
    'B': {'reward': -0.5, 'actions': {'up': 'B', 'down': 'T', 'left': 'S', 'right': 'B'}},
    'T': {'reward': 100, 'actions': {'up': 'B', 'down': 'T', 'left': 'T', 'right': 'T'}}
}

# Initialize Q-Table
q_table = {
    'E': {'up': 0, 'down': 0, 'left': 0, 'right': 0},
    'T': {'up': 0, 'down': 0, 'left': 0, 'right': 0},
    'S': {'up': 0, 'down': 0, 'left': 0, 'right': 0},
    'B': {'up': 0, 'down': 0, 'left': 0, 'right': 0}
}

# Hyperparameters
alpha = 0.1  # Learning rate
gamma = 1.0  # Discount factor

# Step 1: Start at state S
current_state = 'S'

# Step 2: Choose an action (right, as per example)
action = 'right'

# Step 3: Perform the action and observe the next state
next_state = 'B'  # From S, action 'right' leads to B

# Step 4: Measure the reward
reward = -0.5  # Reward for state B

# Step 5: Update the Q-Table
max_future_q = max(q_table[next_state].values())  # Maximum Q-value for the next state (B)
current_q = q_table[current_state][action]  # Current Q-value for the chosen action (right)

# Q-Learning formula
new_q = (1 - alpha) * current_q + alpha * (reward + gamma * max_future_q)
q_table[current_state][action] = new_q
#Q(S,right)=(1âˆ’0.1)â‹…0+0.1â‹…(âˆ’0.5+1â‹…0)=âˆ’0.05

# Print the updated Q-Table
print("Updated Q-Table:")
for state, actions in q_table.items():
    print(f"{state}: {actions}")


#Task 2:
#â€¢ After 5 completed episodes with epsilon-greedy strategy (ğœ– = 0.05) the Q-Table looks like shown
#below
#â€¢ Compute the first iteration of Q-Learning for episode 6, starting again at (S) by hand
#â€¢ As before: Neglect epsilon-greedy strategy